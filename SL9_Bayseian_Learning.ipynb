{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "Naive Bayes classifiers are a family of classifiers that are quite similar to the linear models discussed previously. However, they tend to be even faster in training. The price paid for this efficiency is that naive Bayes models often provide generalization performance that is slightly worse than that of linear classifiers like **LogisticRegression** and **LinearSVC**.\n",
    "\n",
    "The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collect simple per-class statistics from each feature. There are three kinds of naive Bayes classifiers implemented in *scikit-learn*: [GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), [BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html), and [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). **GaussianNB** can be applied to any continuous data, while **BernoulliNB** assumes binary data and **MultinomialNB** assumes count data (that is, that each feature represents an integer count of something, like how often a word appears in a sentence). **BernoulliNB** and **MultinomialNB** are mostly used in text data classification.\n",
    "\n",
    "## Advantages of Naive Bayes\n",
    "* Very fast to train and to predict\n",
    "* Training procedure is easy to understand\n",
    "* The models work very well with high-dimensional sparse data and are relatively robust to the parameters\n",
    "\n",
    "## Disadvantages of Naive Bayes\n",
    "* Relatively poor generalization performance\n",
    "\n",
    "*Disclaimer*: Much of the code in this notebook was borrowed from the excellent book [Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do) by Andreas Muller and Sarah Guido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **BernoulliNB** classifier counts how often every feature of each class is not zero. This is most easily understood with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 1, 0, 1],\n",
    "              [1, 0, 1, 1],\n",
    "              [0, 0, 0, 1],\n",
    "              [1, 0, 1, 0]])\n",
    "y = np.array([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have four data points, with four binary features each. There are two classes, 0 and 1. For class 0 (the first and third data points), the first feature is zero two times and nonzero zero times, the second feature is zero one time and nonzero one time, and so on. These same counts are then calculated for the data points in the second class. Counting the nonzero entries per class in essence looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature counts:\n",
      "{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    # iterate over each class\n",
    "    # count (sum) entries of 1 per feature\n",
    "    counts[label] = X[y == label].sum(axis=0)\n",
    "print(\"Feature counts:\\n{}\".format(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other two naive Bayes models, **MultinomialNB** and **GaussianNB**, are slightly different in what kinds of statistics they compute. MultinomialNB takes into account the average value of each feature for each class, while GaussianNB stores the average value as well as the standard deviation of each feature for each class. \n",
    "\n",
    "To make a prediction, a data point is compared to the statistics for each of the classes, and the best matching class is predicted. Interestingly, for both **MultinomialNB** and **BernoulliNB**, this leads to a prediction formula that is of the same form as in the linear models. Unfortunately, `coef_` for the naive Bayes models has a somewhat different meaning than in the linear models, in that `coef_` is not the same as w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianNB\n",
    "Let's apply a **GaussianNB** model to the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.97\n",
      "Accuracy on test set: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=5)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(gnb.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Naive Bayes models are great baseline models and are often used on very large datasets, where training even a linear model might take too long.  However, if accuracy is important, there are generally numerous other models which will have better generalization performance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
