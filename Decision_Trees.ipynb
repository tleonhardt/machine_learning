{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic Competition\n",
    "This Jupyter Notebook examines how to use Python's scikit-learn module to create and train Decision Tree machine learning models.  It does so specifically within the context of the [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic).  \n",
    "\n",
    "[kaggle.com](kaggle.com) is a website which hosts machine learning competitions where experts can win cash prizes and those new to machine learning can learn the ropes in a practical manner by exploring using various techniques to solve the same problem.  \n",
    "\n",
    "Kaggle's [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition is their most popular competition for beginners and those new to data science and machine learning.  It contains numerous tutorials and examples.\n",
    "\n",
    "The particular solution presented here is based heavily on the excellent free Kaggle Python tutorial from [DataCamp](https://campus.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning).\n",
    "\n",
    "When the Titanic sank, 1502 of the 2224 passengers and crew were killed. One of the main reasons for this high level of casualties was the lack of lifeboats on this self-proclaimed \"unsinkable\" ship.\n",
    "\n",
    "Those that have seen the movie know that some individuals were more likely to survive the sinking (lucky Rose) than others (poor Jack). In this example, you will learn how to apply machine learning techniques to predict a passenger's chance of surviving using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Datasets\n",
    "First we can use Python's builtin os package to determine if the training and test set CSV files already exist locally.\n",
    "\n",
    "If they do not exist, then we can use Python's urllib package to download them from DataCamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os and urllib\n",
    "import os\n",
    "\n",
    "# For Python 3.x the import should be urllib.request, but for Python 2.x it should just be urllib\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure data directory exists\n",
    "data_dir = 'data/kaggle/titanic'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filenames for the train and test datasets\n",
    "train_csv = os.path.join(data_dir, 'train.csv')\n",
    "test_csv = os.path.join(data_dir, 'test.csv')\n",
    "\n",
    "# If the data doesn't already exist locally, then download it using urlretrieve\n",
    "if not os.path.isfile(train_csv):\n",
    "    train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "    urlretrieve (train_url, train_csv)\n",
    "if not os.path.isfile(test_csv):\n",
    "    test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "    urlretrieve (test_url, test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "[pandas](http://pandas.pydata.org) is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "One things pandas excels at is reading data from and writing data to pretty much any common format including CSV files, SQL databases, JSON, Excel files, MATLAB, etc.\n",
    "\n",
    "The primary data structure provided by pandas is the DataFrame, which is sort of like a hybrid between an Excel spreadsheet and a SQL database table.  It is VERY powerful in terms of the capabilities provided by this class.  But there is a bit of a learning curve for newcomers.\n",
    "\n",
    "## Get the data with Pandas\n",
    "Let's start with loading in the training and testing set into your Python environment. You will use the training set to build your model, and the test set to validate it. The data is stored on the web as *csv* files; their URLs are already available as character strings in the sample code. You can load this data with the **read_csv()** method from the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the train and test datasets from the local CSV files to create two DataFrames\n",
    "train = pd.read_csv(train_csv)\n",
    "test = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first few rows of the training dataset\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first few rows of the test dataset\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Your Data\n",
    "Before starting with the actual analysis, it's important to understand the structure of your data. Both *test* and *train* are DataFrame objects, the way pandas represent datasets. You can easily explore a DataFrame using the **.describe()** method. **.describe()** summarizes the columns/features of the DataFrame, including the count of observations, mean, max and so on. Another useful trick is to look at the dimensions of the DataFrame. This is done by requesting the **.shape** attribute of your DataFrame object. (ex. your_data.shape)\n",
    "\n",
    "The training and test set are already available in the workspace, as *train* and *test*. Apply **.describe()** method and print the .shape attribute of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame.describe() generates various various summary statistics, automatically excluding NaN or missing values\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This means the training set has 891 observations with 12 variables each\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Female vs Male\n",
    "How many people in your training set survived the disaster with the Titanic? To see this, you can use the **value_counts()** method in combination with standard bracket notation to select a single column of a DataFrame:\n",
    "    # absolute numbers\n",
    "    train[\"Survived\"].value_counts()\n",
    "\n",
    "    # percentages\n",
    "    train[\"Survived\"].value_counts(normalize = True)\n",
    "If you run these commands in the console, you'll see that 549 individuals died (62%) and 342 survived (38%). A simple way to predict heuristically could be: \"majority wins\". This would mean that you will predict every unseen observation to not survive.\n",
    "\n",
    "To dive in a little deeper we can perform similar counts and percentage calculations on subsets of the Survived column. For example, maybe gender could play a role as well? You can explore this using the **.value_counts()** method for a two-way comparison on the number of males and females that survived, with this syntax:\n",
    "    \n",
    "    train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts()\n",
    "    train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts()\n",
    "To get proportions, you can again pass in the argument normalize = True to the **.value_counts()** method.\n",
    "\n",
    "The results below show that 81% of the men died, but only 26% of the women died.  So gender matters tremendously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    549\n",
      "1    342\n",
      "Name: Survived, dtype: int64\n",
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: Survived, dtype: float64\n",
      "0    468\n",
      "1    109\n",
      "Name: Survived, dtype: int64\n",
      "1    233\n",
      "0     81\n",
      "Name: Survived, dtype: int64\n",
      "\n",
      "Male survival rates:\n",
      "0    0.811092\n",
      "1    0.188908\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Female survival rates:\n",
      "1    0.742038\n",
      "0    0.257962\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Passengers that survived vs passengers that passed away\n",
    "print(train[\"Survived\"].value_counts())\n",
    "\n",
    "# As proportions\n",
    "print(train[\"Survived\"].value_counts(normalize=True))\n",
    "\n",
    "# Males that survived vs males that passed away\n",
    "print(train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts())\n",
    "\n",
    "# Females that survived vs Females that passed away\n",
    "print(train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts())\n",
    "\n",
    "# Normalized male survival\n",
    "print(\"\\nMale survival rates:\\n{}\".format(train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize=True)))\n",
    "\n",
    "# Normalized female survival\n",
    "print(\"\\nFemale survival rates:\\n{}\".format(train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does age play a role?\n",
    "Another variable that could influence survival is age; since it's probable that children were saved first. You can test this by creating a new column with a categorical variable *Child*. *Child* will take the value 1 in cases where age is less than 18, and a value of 0 in cases where age is greater than or equal to 18.\n",
    "\n",
    "To add this new variable you need to do two things (i) create a new column, and (ii) provide the values for each observation (i.e., row) based on the age of the passenger.\n",
    "\n",
    "Adding a new column with Pandas in Python is easy and can be done via the following syntax:\n",
    "\n",
    "    your_data[\"new_var\"] = 0\n",
    "This code would create a new column in the *train* DataFrame titled *new_var* with *0* for each observation.\n",
    "\n",
    "To set the values based on the age of the passenger, you make use of a boolean test inside the square bracket operator. With the **[]-operator** you create a subset of rows and assign a value to a certain variable of that subset of observations. For example,\n",
    "\n",
    "    train[\"new_var\"][train[\"Fare\"] > 10] = 1\n",
    "would give a value of *1* to the variable *new_var* for the subset of passengers whose fares greater than 10. Remember that *new_var* has a value of *0* for all other values (including missing values).\n",
    "\n",
    "The data below shows that 54% of children survived, but only 38% of adults survived.  So yes, age does play a role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rates for children:\n",
      "1    0.539823\n",
      "0    0.460177\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Survival rates for adults:\n",
      "0    0.638817\n",
      "1    0.361183\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Added a new column called Child in the train data frame that initially takes the value 0 for all observations\n",
    "train[\"Child\"] = float(0)\n",
    "\n",
    "# Assign 1 to passengers under 18\n",
    "train.loc[train[\"Age\"] < 18, \"Child\"] = 1\n",
    "\n",
    "# Print normalized Survival Rates for passengers under 18\n",
    "print(\"Survival rates for children:\\n{}\".format(train[\"Survived\"][train[\"Child\"] == 1].value_counts(normalize = True)))\n",
    "\n",
    "# Print normalized Survival Rates for passengers 18 or older\n",
    "print(\"\\nSurvival rates for adults:\\n{}\".format(train[\"Survived\"][train[\"Child\"] == 0].value_counts(normalize = True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Decision Trees\n",
    "In the previous sections, you did all the slicing and dicing yourself to find subsets that have a higher chance of surviving. A decision tree automates this process for you and outputs a classification model or classifier.\n",
    "\n",
    "Conceptually, the decision tree algorithm starts with all the data at the root node and scans all the variables for the best one to split on. Once a variable is chosen, you do the split and go down one level (or one node) and repeat. The final nodes at the bottom of the decision tree are known as terminal nodes, and the majority vote of the observations in that node determine how to predict for new observations that end up in that terminal node.\n",
    "\n",
    "First, let's import the necessary libraries ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Import 'tree' from scikit-learn library\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Formatting Your Data\n",
    "Before you can begin constructing your trees you need to get your hands dirty and clean the data so that you can use all the features available to you. In the first chapter, we saw that the Age variable had some missing value. Missingness is a whole subject with and in itself, but we will use a simple imputation technique where we substitute each missing value with the median of the all present values.\n",
    "\n",
    "    train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "Another problem is that the Sex and Embarked variables are categorical but in a non-numeric format. Thus, we will need to assign each class a unique integer so that Python can handle the information. Embarked also has some missing values which you should impute witht the most common class of embarkation, which is \"S\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    577\n",
      "1    314\n",
      "Name: Sex, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing Age values with median age from training set\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "test[\"Age\"] = test[\"Age\"].fillna(train[\"Age\"].median())\n",
    "\n",
    "# Convert the male and female groups to integer form by replacing \"male\" with 0 and \"female\" with 1\n",
    "train.loc[train[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "train.loc[train[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "test.loc[test[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "test.loc[test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "\n",
    "# Print value counts for the Sex and Embarked columns\n",
    "print(train[\"Sex\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    646\n",
      "1    168\n",
      "2     77\n",
      "Name: Embarked, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute the Embarked variable\n",
    "train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n",
    "test[\"Embarked\"] = test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "# Convert the Embarked classes to integer form\n",
    "train.loc[train[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "train.loc[train[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "train.loc[train[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "test.loc[test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "test.loc[test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "test.loc[test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "\n",
    "print(train[\"Embarked\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute any missing values in Fare\n",
    "train[\"Fare\"] = train[\"Fare\"].fillna(train[\"Fare\"].median())\n",
    "test[\"Fare\"] = test[\"Fare\"].fillna(train[\"Fare\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your first decision tree\n",
    "You will use the **scikit-learn** and **numpy** libraries to build your first decision tree. **scikit-learn** can be used to create **tree** objects from the **DecisionTreeClassifier** class. The methods that we will use take **numpy** arrays as inputs and therefore we will need to create those from the **DataFrame** that we already have. We will need the following to build a decision tree\n",
    "* target: A one-dimensional numpy array containing the target/response from the train data. (Survival in your case)\n",
    "* features: A multidimensional numpy array containing the features/predictors from the train data. (ex. Sex, Age)\n",
    "Take a look at the sample code below to see what this would look like:\n",
    "\n",
    "    target = train[\"Survived\"].values\n",
    "    \n",
    "    features = train[[\"Sex\", \"Age\"]].values\n",
    "    \n",
    "    my_tree = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    my_tree = my_tree.fit(features, target)\n",
    "\n",
    "One way to quickly see the result of your decision tree is to see the importance of the features that are included. This is done by requesting the **.feature_importances_** attribute of your tree object. Another quick metric is the mean accuracy that you can compute using the **.score()** function with *features_one* and *target* as arguments.\n",
    "\n",
    "Ok, time for you to build your first decision tree in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 22.0, 7.25],\n",
       "       [1, 1, 38.0, 71.2833],\n",
       "       [3, 1, 26.0, 7.925],\n",
       "       ..., \n",
       "       [3, 1, 28.0, 23.45],\n",
       "       [1, 0, 26.0, 30.0],\n",
       "       [3, 0, 32.0, 7.75]], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the target and features numpy arrays: target, features_one\n",
    "target = train[\"Survived\"].values\n",
    "columns_one = [\"Pclass\", \"Sex\", \"Age\", \"Fare\"]\n",
    "features_one = train[columns_one].values\n",
    "features_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12629524  0.31274009  0.23147427  0.32949039]\n",
      "0.977553310887\n"
     ]
    }
   ],
   "source": [
    "# Fit your first decision tree: my_tree_one\n",
    "my_tree_one = tree.DecisionTreeClassifier()\n",
    "my_tree_one = my_tree_one.fit(features_one, target)\n",
    "\n",
    "# Look at the importance and score of the included features\n",
    "print(my_tree_one.feature_importances_)\n",
    "print(my_tree_one.score(features_one, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting your decision tree\n",
    "The **feature_importances_** attribute make it simple to interpret the significance of the predictors you include. \n",
    "Based on your decision tree, what variable plays the most important role in determining whether or not a passenger survived?\n",
    "\n",
    "Based on this decision tree, the **Fare** variable plays the most important role, but it is nearly tied with **Age** in importance.\n",
    "\n",
    "Based on the score, we can see that our decision tree fit based on the training set predicts approximately 98% of the values in the training set correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and submit to Kaggle\n",
    "To send a submission to Kaggle you need to predict the survival rates for the observations in the test set. Luckily, with our decision tree, we can make use of some simple functions to \"generate\" our answer without having to manually perform subsetting.\n",
    "\n",
    "First, you make use of the **.predict()** method. You provide it the model (*my_tree_one*), the values of features from the dataset for which predictions need to be made (*test*). To extract the features we will need to create a numpy array in the same way as we did when training the model. However, we need to take care of a small but important problem first. There is a missing value in the Fare feature that needs to be imputed.\n",
    "\n",
    "Next, you need to make sure your output is in line with the submission requirements of Kaggle: a csv file with exactly 418 entries and two columns: *PassengerId* and *Survived*. Then use the code provided to make a new data frame using **DataFrame()**, and create a csv file using **to_csv()** method from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure solution directory exists\n",
    "solution_dir = 'solutions/kaggle/titanic'\n",
    "if not os.path.isdir(solution_dir):\n",
    "    os.makedirs(solution_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 1)\n"
     ]
    }
   ],
   "source": [
    "# Impute the missing value with the median\n",
    "test.Fare.fillna(test.Fare.median())\n",
    "\n",
    "# Extract the features from the test set: Pclass, Sex, Age, and Fare.\n",
    "test_features = test[columns_one].values\n",
    "\n",
    "# Make your prediction using the test set\n",
    "my_prediction = my_tree_one.predict(test_features)\n",
    "\n",
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\n",
    "PassengerId = np.array(test[\"PassengerId\"]).astype(int)\n",
    "my_solution = pd.DataFrame(my_prediction, PassengerId, columns = [\"Survived\"])\n",
    "\n",
    "# Check that your data frame has 418 entries\n",
    "print(my_solution.shape)\n",
    "\n",
    "# Write your solution to a csv file\n",
    "my_solution.to_csv(os.path.join(solution_dir, \"decision_tree_one.csv\"), index_label = [\"PassengerId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well does that first decision tree do on the test set?\n",
    "\n",
    "This basic solution achieves a score of 0.75120 on the test set.  So it predicts approximately 75% of the values in the test set correctly.  But from earlier we saw that this decision tree predicted approximately 98% of the values in the training set correctly?  \n",
    "\n",
    "So why did we do so much better predicting values in the training set as compared to the test set?\n",
    "**Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and how to control it\n",
    "When you created your first decision tree the default arguments for **max_depth** and **min_samples_split** were set to **None**. This means that no limit on the depth of your tree was set. That's a good thing right? Not so fast. We are likely overfitting. This means that while your model describes the training data extremely well, it doesn't generalize to new data, which is frankly the point of prediction. Just look at the Kaggle submission results for the simple model based on Gender and the complex decision tree. Which one does better?\n",
    "* A gender-only based model achieves a score of 0.765, which is better than our first decision tree, ouch!\n",
    "\n",
    "Maybe we can improve the overfit model by making a less complex model? In **DecisionTreeRegressor**, the depth of our model is defined by two parameters: - the **max_depth** parameter determines when the splitting up of the decision tree stops. - the **min_samples_split** parameter monitors the amount of observations in a bucket. If a certain threshold is not reached (e.g minimum 10 passengers) no further splitting can be done.\n",
    "\n",
    "By limiting the complexity of your decision tree you will increase its generality and thus its usefulness for prediction!\n",
    "\n",
    "It may also help to add additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.905723905724\n"
     ]
    }
   ],
   "source": [
    "# Create a new array with the added features: features_two\n",
    "columns_two = [\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]\n",
    "features_two = train[columns_two].values\n",
    "\n",
    "#Control overfitting by setting \"max_depth\" to 10 and \"min_samples_split\" to 5 : my_tree_two\n",
    "max_depth = 10\n",
    "min_samples_split = 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split, random_state = 1)\n",
    "my_tree_two.fit(features_two, target)\n",
    "\n",
    "#Print the score of the new decison tree\n",
    "print(my_tree_two.score(features_two, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make your prediction using the test set\n",
    "test_features_two = test[columns_two].values\n",
    "predition_two = my_tree_two.predict(test_features_two)\n",
    "predition_two.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\n",
    "solution_two = pd.DataFrame(predition_two, PassengerId, columns = [\"Survived\"])\n",
    "\n",
    "# Check that your data frame has 418 entries\n",
    "print(solution_two.shape)\n",
    "\n",
    "# Write your solution to a csv file with the name my_solution.csv\n",
    "solution_two.to_csv(os.path.join(solution_dir,\"decision_tree_two.csv\"), index_label = [\"PassengerId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well did our attempts at preventing overfitting help?\n",
    "This second solution achieves a higher score of 0.76555 on the test set, even though it had a lower score of 0.906 on the training set. So it predicts approximately 76.6% of the values in the test set correctly. \n",
    "\n",
    "This is a little bit better than before we attempted to prevent overfitting.  But it is still only par with a pure gender-based model.  So there is still a lot of room for improvement.\n",
    "\n",
    "How can we do better?  One way is to spend a little bit of time on feature engineering ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-engineering for our Titanic data set\n",
    "Data Science is an art that benefits from a human element. Enter feature engineering: creatively engineering your own features by combining the different existing variables.\n",
    "\n",
    "While feature engineering is a discipline in itself, too broad to be covered here in detail, you will have a look at a simple example by creating your own new predictive attribute: *family_size*.\n",
    "\n",
    "A valid assumption is that larger families need more time to get together on a sinking ship, and hence have lower probability of surviving. Family size is determined by the variables SibSp and Parch, which indicate the number of family members a certain passenger is traveling with. So when doing feature engineering, you add a new variable family_size, which is the sum of SibSp and Parch plus one (the observation itself), to the test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.890011223345\n"
     ]
    }
   ],
   "source": [
    "# Create train_two with the newly defined feature\n",
    "train_two = train.copy()\n",
    "train_two[\"family_size\"] = train_two[\"SibSp\"] +  train_two[\"Parch\"] + 1\n",
    "\n",
    "cols_three = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\", \"family_size\", \"Child\"]\n",
    "# Create a new feature set and add the new feature\n",
    "features_three = train_two[cols_three].values\n",
    "\n",
    "\n",
    "#Control overfitting by setting \"max_depth\" to 9 and \"min_samples_split\" to 6\n",
    "max_depth = 9\n",
    "min_samples_split = 6\n",
    "\n",
    "# Define the tree classifier, then fit the model\n",
    "my_tree_three = tree.DecisionTreeClassifier(max_depth = max_depth, min_samples_split = min_samples_split, random_state = 1)\n",
    "my_tree_three.fit(features_three, target)\n",
    "\n",
    "# Print the score of this decision tree\n",
    "print(my_tree_three.score(features_three, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(418,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make your prediction using the test set\n",
    "test_two = test.copy()\n",
    "\n",
    "test_two[\"Child\"] = float(0)\n",
    "test_two.loc[test_two[\"Age\"] < 18, \"Child\"] = 1\n",
    "\n",
    "test_two[\"family_size\"] = test_two[\"SibSp\"] +  test_two[\"Parch\"] + 1\n",
    "test_features_three = test_two[cols_three].values\n",
    "predition_three = my_tree_three.predict(test_features_three)\n",
    "predition_three.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\n",
    "solution_three = pd.DataFrame(predition_three, PassengerId, columns = [\"Survived\"])\n",
    "\n",
    "# Check that your data frame has 418 entries\n",
    "print(solution_three.shape)\n",
    "\n",
    "# Write your solution to a csv file with the name my_solution.csv\n",
    "solution_three.to_csv(os.path.join(solution_dir, \"decision_tree_three.csv\"), index_label = [\"PassengerId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new model has a score of 0.7703, so it predicts about 77% of test set data correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we do better?\n",
    "The Random Forest technique handles the overfitting problem faced with decision trees. It grows multiple (very deep) classification trees using the training set. At the time of prediction, each tree is used to come up with a prediction and every outcome is counted as a vote.  For example, if you have trained 3 trees with 2 saying a passenger in the test set will survive and 1 says he will not, the passenger will be classified as a survivor. This approach of overtraining trees, but having the majority's vote count as the actual classification decision, avoids overfitting.\n",
    "\n",
    "Building a random forest in Python looks almost the same as building a decision tree; so we can jump right to it. There are two key differences, however. Firstly, a different class is used. And second, a new argument is necessary. Also, we need to import the necessary library from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
